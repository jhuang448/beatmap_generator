{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rmpybwysXGV"
   },
   "source": [
    "### osu!nn #7: GAN map flow generator\n",
    "\n",
    "Generate a nice map using a GAN and the data we have gathered until now.\n",
    "\n",
    "Synthesis of \"flowData\"\n",
    "* training_flowData x 10 ~ 99 (Quality: 60+)\n",
    "* rhythmData x 1\n",
    "* momentumData x 1\n",
    "* (Discriminator) x 1\n",
    "* (Generator) x 1\n",
    "\n",
    "Synthesis Time: ~15 mins\n",
    "\n",
    "Last edit: 2019/4/22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellView": "form",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "m8y3rGtQsYP2"
   },
   "source": [
    "#### First of all, let's welcome -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hrXv0rU9sIma"
   },
   "source": [
    "## Cute Sophie!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7S0BwJ_8sLu7"
   },
   "source": [
    "<img src=\"https://ar3.moe/files/sophie.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k2o3TTG4TFpt"
   },
   "source": [
    "In the previous notebook we have predicted... or estimated our rhythm, and now we will be trying to create the new map by imitating the existing dataset, using a Generative Adversial Network (GAN).\n",
    "\n",
    "Note that this GAN is irrelevant to the music, not the time interval, only the coordinates of notes themselves.\n",
    "\n",
    "Probably could get some slider coordinates inbetween? this way it may learn something about slider shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "PJ64L90aVir3"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, re, subprocess, json\n",
    "from datetime import datetime\n",
    "\n",
    "# Tensorflow 2.0 enables eager automatically\n",
    "try:\n",
    "    tf.enable_eager_execution();\n",
    "    tfe = tf.contrib.eager;\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some parameters related to GAN training.\n",
    "\n",
    "May or may not affect the result; haven't experimented a lot about these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAN_PARAMS = {\n",
    "    \"divisor\" : 4,\n",
    "    \"good_epoch\" : 6,\n",
    "    \"max_epoch\" : 25,\n",
    "    \"note_group_size\" : 10,\n",
    "    \"g_epochs\" : 7,\n",
    "    \"c_epochs\" : 3,\n",
    "    \"g_batch\" : 50,\n",
    "    \"g_input_size\" : 50,\n",
    "    \"c_true_batch\" : 50,\n",
    "    \"c_false_batch\" : 10,\n",
    "    \"slider_max_ticks\" : 8,\n",
    "    \"next_from_slider_end\" : False\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "hBkaqxc2TsrW"
   },
   "source": [
    "#### Import the rhythm data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_npz(fn):\n",
    "    with np.load(fn) as data:\n",
    "        objs = data[\"objs\"];\n",
    "        obj_indices = [i for i,k in enumerate(objs) if k == 1];\n",
    "        predictions = data[\"predictions\"];\n",
    "        momenta = data[\"momenta\"];\n",
    "        ticks = data[\"ticks\"];\n",
    "        timestamps = data[\"timestamps\"];\n",
    "        sv = data[\"sv\"];\n",
    "        dist_multiplier = data[\"dist_multiplier\"];\n",
    "    return objs, obj_indices, predictions, momenta, ticks, timestamps, sv, dist_multiplier;\n",
    "\n",
    "unfiltered_objs, obj_indices, unfiltered_predictions, unfiltered_momenta, unfiltered_ticks, unfiltered_timestamps, unfiltered_sv, dist_multiplier = read_npz(\"rhythm_data.npz\");\n",
    "\n",
    "first_step_objs =        unfiltered_objs[obj_indices];\n",
    "first_step_predictions = unfiltered_predictions[obj_indices];\n",
    "first_step_momenta =     unfiltered_momenta[obj_indices];\n",
    "first_step_ticks =       unfiltered_ticks[obj_indices];\n",
    "first_step_timestamps =  unfiltered_timestamps[obj_indices];\n",
    "first_step_sv =          unfiltered_sv[obj_indices];\n",
    "\n",
    "momentum_multiplier = 1.0;\n",
    "angular_momentum_multiplier = 1.0;\n",
    "\n",
    "first_step_is_slider = first_step_predictions[:, 2];\n",
    "first_step_is_spinner = first_step_predictions[:, 3];\n",
    "first_step_is_sliding = first_step_predictions[:, 4];\n",
    "first_step_is_spinning = first_step_predictions[:, 5];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert notes with is_slider flag to sliders\n",
    "# if there is next note, slide to next note\n",
    "# else, slide for 4 ticks\n",
    "\n",
    "# Problems:\n",
    "# - how to deal with momenta? (change the slider speed! which is obviously not good)\n",
    "# - do we use AM?\n",
    "# - do we use is_sliding?\n",
    "# - do we use the slider model? it's heavily overfit... (try with some other dataset, other than sota!!!)\n",
    "# - does the shape cause overlapping? add some penalty loss? let it learn from classifier? ...\n",
    "# - and many more!\n",
    "skip_this = False;\n",
    "new_obj_indices = [];\n",
    "slider_ticks = [];\n",
    "slider_max_ticks = GAN_PARAMS[\"slider_max_ticks\"];\n",
    "for i in range(len(first_step_objs)):\n",
    "    if skip_this:\n",
    "        first_step_is_slider[i] = 0;\n",
    "        skip_this = False;\n",
    "        continue;\n",
    "    if first_step_is_slider[i]: # this one is a slider!!\n",
    "        if i == first_step_objs.shape[0]-1: # Last Note.\n",
    "            new_obj_indices.append(i);\n",
    "            slider_ticks.append(slider_max_ticks);\n",
    "            continue;\n",
    "        if first_step_ticks[i+1] >= first_step_ticks[i] + slider_max_ticks + 1: # too long! end here\n",
    "            new_obj_indices.append(i);\n",
    "            slider_ticks.append(slider_max_ticks);\n",
    "        else:\n",
    "            skip_this = True;\n",
    "            new_obj_indices.append(i);\n",
    "            slider_ticks.append(max(1, first_step_ticks[i+1] - first_step_ticks[i]));\n",
    "    else: # not a slider!\n",
    "        new_obj_indices.append(i);\n",
    "        slider_ticks.append(0);\n",
    "\n",
    "# Filter the removed objects out!\n",
    "objs =        first_step_objs[new_obj_indices];\n",
    "predictions = first_step_predictions[new_obj_indices];\n",
    "momenta =     first_step_momenta[new_obj_indices];\n",
    "ticks =       first_step_ticks[new_obj_indices];\n",
    "timestamps =  first_step_timestamps[new_obj_indices];\n",
    "is_slider =   first_step_is_slider[new_obj_indices];\n",
    "is_spinner =  first_step_is_spinner[new_obj_indices];\n",
    "is_sliding =  first_step_is_sliding[new_obj_indices];\n",
    "is_spinning = first_step_is_spinning[new_obj_indices];\n",
    "sv =          first_step_sv[new_obj_indices];\n",
    "slider_ticks = np.array(slider_ticks);\n",
    "\n",
    "# get divisor from GAN_PARAMS\n",
    "divisor = GAN_PARAMS[\"divisor\"];\n",
    "\n",
    "# should be slider length each tick, which is usually SV * SMP * 100 / 4\n",
    "# e.g. SV 1.6, timing section x1.00, 1/4 divisor, then slider_length_base = 40\n",
    "slider_length_base = sv // divisor;\n",
    "\n",
    "# these data must be kept consistent with the sliderTypes in load_map.js\n",
    "slider_types = np.random.randint(0, 5, is_slider.shape).astype(int); # needs to determine the slider types!! also it is 5!!!\n",
    "slider_type_rotation = np.array([0, -0.40703540572409336, 0.40703540572409336, -0.20131710837464062, 0.20131710837464062]);\n",
    "slider_cos = np.cos(slider_type_rotation);\n",
    "slider_sin = np.sin(slider_type_rotation);\n",
    "\n",
    "slider_cos_each = slider_cos[slider_types];\n",
    "slider_sin_each = slider_sin[slider_types];\n",
    "\n",
    "# this is vector length! I should change the variable name probably...\n",
    "slider_type_length = np.array([1.0, 0.97, 0.97, 0.97, 0.97]);\n",
    "\n",
    "slider_lengths = np.array([slider_type_length[int(k)] * slider_length_base[i] for i, k in enumerate(slider_types)]) * slider_ticks;\n",
    "\n",
    "# print(slider_lengths.shape)\n",
    "# print(timestamps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For distances starting from slider ends\n",
    "tick_lengths_pre = (timestamps[1:] - timestamps[:-1]) / (ticks[1:] - ticks[:-1]);\n",
    "tick_lengths = np.concatenate([tick_lengths_pre, [tick_lengths_pre[-1]]]);\n",
    "timestamps_note_end = timestamps + slider_ticks * tick_lengths;\n",
    "\n",
    "timestamps_plus_1 = np.concatenate([timestamps[1:], timestamps[-1:] + (timestamps[-1:] - timestamps[-2:-1])])\n",
    "\n",
    "if GAN_PARAMS[\"next_from_slider_end\"]:\n",
    "    timestamps_after = timestamps_plus_1 - timestamps_note_end;\n",
    "    timestamps_before = np.concatenate([[6662], timestamps_after[:-1]]); # why 6662????\n",
    "    note_distances = timestamps_before * momenta[:, 0] * momentum_multiplier;\n",
    "else:\n",
    "    timestamps_after = timestamps_plus_1 - timestamps;\n",
    "    timestamps_before = np.concatenate([[4777], timestamps_after[:-1]]); # why 4777????\n",
    "    note_distances = timestamps_before * momenta[:, 0] * momentum_multiplier;\n",
    "note_angles = timestamps_before * momenta[:, 1] * angular_momentum_multiplier;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_slider = predictions[:, 2];\n",
    "is_sliding = predictions[:, 4];\n",
    "#print(is_slider * is_sliding - is_slider); # is all 0!!\n",
    "# print(is_slider * is_sliding);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting functions (only for debugging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "VAkAvc3ATskA"
   },
   "outputs": [],
   "source": [
    "from plthelper import MyLine, plot_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data for classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use flow_dataset.npz\n"
     ]
    }
   ],
   "source": [
    "root = \"mapdata/\";\n",
    "\n",
    "chunk_size = GAN_PARAMS[\"note_group_size\"];\n",
    "step_size = 5;\n",
    "\n",
    "max_x = 512;\n",
    "max_y = 384;\n",
    "\n",
    "# \"TICK\", \"TIME\", \"TYPE\", \"X\", \"Y\", \"IN_DX\", \"IN_DY\", \"OUT_DX\", \"OUT_DY\"\n",
    "def read_map_npz(file_path):\n",
    "    with np.load(file_path) as data:\n",
    "        flow_data = data[\"flow\"];\n",
    "    return flow_data;\n",
    "\n",
    "# TICK, TIME, TYPE, X, Y, IN_DX, IN_DY, OUT_DX, OUT_DY\n",
    "def read_maps():\n",
    "    result = [];\n",
    "    for file in os.listdir(root):\n",
    "        if file.endswith(\".npz\"):\n",
    "            #print(os.path.join(root, file));\n",
    "            flow_data = read_map_npz(os.path.join(root, file));\n",
    "            for i in range(0, (flow_data.shape[0] - chunk_size) // step_size):\n",
    "                chunk = flow_data[i * step_size:i * step_size + chunk_size];\n",
    "                result.append(chunk);\n",
    "                \n",
    "    # normalize the TICK col and remove TIME col\n",
    "    result = np.array(result)[:, :, [0, 2, 3, 4, 5, 6, 7, 8, 9, 10]];\n",
    "    result[:, :, 0] %= divisor;\n",
    "    result[:, :, 2] /= max_x;\n",
    "    result[:, :, 3] /= max_y;\n",
    "    result[:, :, 8] /= max_x;\n",
    "    result[:, :, 9] /= max_y;\n",
    "    \n",
    "    # TICK, TYPE, X, Y, IN_DX, IN_DY, OUT_DX, OUT_DY, END_X, END_Y\n",
    "    # only use X,Y,OUT_DX,OUT_DY,END_X,END_Y\n",
    "    result = np.array(result)[:, :, [2, 3, 6, 7, 8, 9]];\n",
    "    return result;\n",
    "\n",
    "# The default dataset so people don't have to come up with a whole dataset to use this.\n",
    "# To save the flow data to a flow_dataset.npz, it is simple - just run the following after reading maps:\n",
    "# np.savez_compressed(\"flow_dataset\", maps = maps);\n",
    "try:\n",
    "    maps = read_maps();\n",
    "    labels = np.ones(maps.shape[0]);\n",
    "except:\n",
    "    print('use flow_dataset.npz')\n",
    "    with np.load(\"flow_dataset.npz\") as flow_dataset:\n",
    "        maps = flow_dataset[\"maps\"];\n",
    "        labels = np.ones(maps.shape[0]);\n",
    "\n",
    "order2 = np.argsort(np.random.random(maps.shape[0]));\n",
    "special_train_data = maps[order2];\n",
    "special_train_labels = labels[order2];\n",
    "# order3 = np.argsort(np.random.random(false_maps.shape[0]));\n",
    "# special_false_data = false_maps[order2];\n",
    "# special_false_labels = false_labels[order2];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the classifier model.\n",
    "\n",
    "The model structure can be probably optimized... while I currently have no good idea about this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras;\n",
    "\n",
    "def build_classifier_model():\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.SimpleRNN(64, input_shape=(special_train_data.shape[1], special_train_data.shape[2])),\n",
    "        keras.layers.Dense(64),# activation=tf.nn.elu, input_shape=(train_data.shape[1],)),\n",
    "        keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(64, activation=tf.nn.tanh),\n",
    "        keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(1, activation=tf.nn.tanh),\n",
    "        keras.layers.Lambda(lambda x: (x+1)/2, output_shape=(1,)),\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        optimizer = tf.optimizers.Adam(0.001) #Adamoptimizer?\n",
    "    except:\n",
    "        optimizer = tf.train.AdamOptimizer(0.001) #Adamoptimizer?\n",
    "\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=[keras.metrics.mae])\n",
    "    return model\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qutT_fkl_CBc"
   },
   "source": [
    "Functions for map generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "gxPTb-kt_N5m"
   },
   "outputs": [],
   "source": [
    "from tfhelper import *\n",
    "\n",
    "# A regularizer to keep the map inside the box.\n",
    "# It's so the sliders and notes don't randomly fly out of the screen!\n",
    "def inblock_loss(vg):\n",
    "    wall_var_l = tf.where(tf.less(vg, 0.2), tf.square(0.3 - vg), 0 * vg);\n",
    "    wall_var_r = tf.where(tf.greater(vg, 0.8), tf.square(vg - 0.7), 0 * vg);\n",
    "    return tf.reduce_mean(tf.reduce_mean(wall_var_l + wall_var_r, axis=2), axis=1);\n",
    "\n",
    "def inblock_trueness(vg):\n",
    "    wall_var_l = tf.cast(tf.less(vg, 0), tf.float32);\n",
    "    wall_var_r = tf.cast(tf.greater(vg, 1), tf.float32);\n",
    "    return tf.reduce_mean(tf.reduce_mean(wall_var_l + wall_var_r, axis=2), axis=1);\n",
    "\n",
    "def cut_map_chunks(c):\n",
    "    r = [];\n",
    "    for i in range(0, (c.shape[0] - chunk_size) // step_size):\n",
    "        chunk = c[i * step_size:i * step_size + chunk_size];\n",
    "        r.append(chunk);\n",
    "    return tf.stack(r);\n",
    "\n",
    "def construct_map_with_sliders(var_tensor, extvar=[]):\n",
    "    var_tensor = tf.cast(var_tensor, tf.float32);\n",
    "    var_shape = var_tensor.shape;\n",
    "    wall_l = 0.15;\n",
    "    wall_r = 0.85;\n",
    "    x_max = 512;\n",
    "    y_max = 384;\n",
    "    out = [];\n",
    "    cp = tf.constant([256, 192, 0, 0]);\n",
    "    phase = 0;\n",
    "    half_tensor = var_shape[1]//4;\n",
    "    \n",
    "    # length multiplier\n",
    "    if \"length_multiplier\" in extvar:\n",
    "        length_multiplier = extvar[\"length_multiplier\"];\n",
    "    else:\n",
    "        length_multiplier = 1;\n",
    "\n",
    "    # notedists\n",
    "    if \"begin\" in extvar:\n",
    "        begin_offset = extvar[\"begin\"];\n",
    "    else:\n",
    "        begin_offset = 0;\n",
    "    \n",
    "#     note_distances_now = length_multiplier * np.expand_dims(note_distances[begin_offset:begin_offset+half_tensor], axis=0);\n",
    "#     note_angles_now = np.expand_dims(note_angles[begin_offset:begin_offset+half_tensor], axis=0);\n",
    "    \n",
    "    relevant_tensors = extvar[\"relevant_tensors\"];\n",
    "    relevant_is_slider =      relevant_tensors[\"is_slider\"];\n",
    "    relevant_slider_lengths = relevant_tensors[\"slider_lengths\"];\n",
    "    relevant_slider_types =   relevant_tensors[\"slider_types\"];\n",
    "    relevant_slider_cos =     relevant_tensors[\"slider_cos_each\"];\n",
    "    relevant_slider_sin =     relevant_tensors[\"slider_sin_each\"];\n",
    "    relevant_note_distances = relevant_tensors[\"note_distances\"];\n",
    "    relevant_note_angles =    relevant_tensors[\"note_angles\"];\n",
    "    \n",
    "    note_distances_now = length_multiplier * tf.expand_dims(relevant_note_distances, axis=0);\n",
    "    note_angles_now = tf.expand_dims(relevant_note_angles, axis=0);\n",
    "\n",
    "    # init\n",
    "    l = tf.convert_to_tensor(note_distances_now, dtype=\"float32\");\n",
    "    sl = l * 0.7;\n",
    "    sr = tf.convert_to_tensor(note_angles_now, dtype=\"float32\");\n",
    "    \n",
    "    cos_list = var_tensor[:, 0:half_tensor * 2];\n",
    "    sin_list = var_tensor[:, half_tensor * 2:];\n",
    "    len_list = tf.sqrt(tf.square(cos_list) + tf.square(sin_list));\n",
    "    cos_list = cos_list / len_list;\n",
    "    sin_list = sin_list / len_list;\n",
    "    \n",
    "    wall_l = 0.05 * x_max + l * 0.5;\n",
    "    wall_r = 0.95 * x_max - l * 0.5;\n",
    "    wall_t = 0.05 * y_max + l * 0.5;\n",
    "    wall_b = 0.95 * y_max - l * 0.5;\n",
    "    rerand = tf.cast(tf.greater(l, y_max / 2), tf.float32);\n",
    "    not_rerand = tf.cast(tf.less_equal(l, y_max / 2), tf.float32);\n",
    "    \n",
    "    next_from_slider_end = extvar[\"next_from_slider_end\"];\n",
    "\n",
    "    # generate\n",
    "    if \"start_pos\" in extvar:\n",
    "        _pre_px = extvar[\"start_pos\"][0];\n",
    "        _pre_py = extvar[\"start_pos\"][1];\n",
    "        _px = tf.cast(_pre_px, tf.float32);\n",
    "        _py = tf.cast(_pre_py, tf.float32);\n",
    "    else:\n",
    "        _px = tf.cast(256, tf.float32);\n",
    "        _py = tf.cast(192, tf.float32);\n",
    "    \n",
    "    # this is not important since the first position starts at _ppos + Δpos\n",
    "    _x = tf.cast(256, tf.float32);\n",
    "    _y = tf.cast(192, tf.float32);\n",
    "    \n",
    "    outputs = tf.TensorArray(tf.float32, half_tensor)\n",
    "\n",
    "    for k in range(half_tensor):\n",
    "        # r_max = 192, r = 192 * k, theta = k * 10\n",
    "        rerand_x = 256 + 256 * var_tensor[:, k];\n",
    "        rerand_y = 192 + 192 * var_tensor[:, k + half_tensor*2];\n",
    "\n",
    "        delta_value_x = l[:, k] * cos_list[:, k];\n",
    "        delta_value_y = l[:, k] * sin_list[:, k];\n",
    "\n",
    "        # It is tensor calculation batched 8~32 each call, so if/else do not work here.\n",
    "        wall_value_l =    tf.cast(tf.less(_px, wall_l[:, k]), tf.float32);\n",
    "        wall_value_r =    tf.cast(tf.greater(_px, wall_r[:, k]), tf.float32);\n",
    "        wall_value_xmid = tf.cast(tf.greater(_px, wall_l[:, k]), tf.float32) * tf.cast(tf.less(_px, wall_r[:, k]), tf.float32);\n",
    "        wall_value_t =    tf.cast(tf.less(_py, wall_t[:, k]), tf.float32);\n",
    "        wall_value_b =    tf.cast(tf.greater(_py, wall_b[:, k]), tf.float32);\n",
    "        wall_value_ymid = tf.cast(tf.greater(_py, wall_t[:, k]), tf.float32) * tf.cast(tf.less(_py, wall_b[:, k]), tf.float32);\n",
    "\n",
    "        x_delta = tf.abs(delta_value_x) * wall_value_l - tf.abs(delta_value_x) * wall_value_r + delta_value_x * wall_value_xmid;\n",
    "        y_delta = tf.abs(delta_value_y) * wall_value_t - tf.abs(delta_value_y) * wall_value_b + delta_value_y * wall_value_ymid;\n",
    "\n",
    "        _x = rerand[:, k] * rerand_x + not_rerand[:, k] * (_px + x_delta);\n",
    "        _y = rerand[:, k] * rerand_y + not_rerand[:, k] * (_py + y_delta);\n",
    "#         _x = _px + x_delta;\n",
    "#         _y = _py + y_delta;\n",
    "        \n",
    "        # calculate output vector\n",
    "        \n",
    "        # slider part\n",
    "        sln = relevant_slider_lengths[k];\n",
    "        slider_type = relevant_slider_types[k];\n",
    "        scos = relevant_slider_cos[k];\n",
    "        ssin = relevant_slider_sin[k];\n",
    "        _a = cos_list[:, k + half_tensor];\n",
    "        _b = sin_list[:, k + half_tensor];\n",
    "        # cos(a+θ) = cosa cosθ - sina sinθ\n",
    "        # sin(a+θ) = cosa sinθ + sina cosθ\n",
    "        _oa = _a * scos - _b * ssin;\n",
    "        _ob = _a * ssin + _b * scos;\n",
    "        cp_slider = tf.transpose(tf.stack([_x / x_max, _y / y_max, _oa, _ob, (_x + _a * sln) / x_max, (_y + _b * sln) / y_max]));\n",
    "        _px_slider = tf.cond(next_from_slider_end, lambda: _x + _a * sln, lambda: _x);\n",
    "        _py_slider = tf.cond(next_from_slider_end, lambda: _y + _b * sln, lambda: _y);\n",
    "        \n",
    "        # circle part\n",
    "        _a = rerand[:, k] * cos_list[:, k + half_tensor] + not_rerand[:, k] * cos_list[:, k];\n",
    "        _b = rerand[:, k] * sin_list[:, k + half_tensor] + not_rerand[:, k] * sin_list[:, k];\n",
    "        cp_circle = tf.transpose(tf.stack([_x / x_max, _y / y_max, _a, _b, _x / x_max, _y / y_max]));\n",
    "        _px_circle = _x;\n",
    "        _py_circle = _y;\n",
    "        \n",
    "        outputs = outputs.write(k, tf.where(relevant_is_slider[k], cp_slider, cp_circle))\n",
    "        _px = tf.where(tf.cast(relevant_is_slider[k], tf.bool), _px_slider, _px_circle)\n",
    "        _py = tf.where(tf.cast(relevant_is_slider[k], tf.bool), _py_slider, _py_circle)\n",
    "\n",
    "    return tf.transpose(outputs.stack(), [1, 0, 2]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.utils import losses_utils\n",
    "from tensorflow.python.keras.losses import LossFunctionWrapper\n",
    "\n",
    "# Loss functions and mapping layer, to adapt to TF 2.0\n",
    "class GenerativeCustomLoss(LossFunctionWrapper):\n",
    "    def __init__(self,\n",
    "        reduction=losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE,\n",
    "        name='generative_custom_loss'):\n",
    "        \n",
    "        def loss_function_for_generative_model(y_true, y_pred):\n",
    "            classification = y_pred;\n",
    "            loss1 = 1 - tf.reduce_mean(classification, axis=1);\n",
    "            return loss1;\n",
    "        \n",
    "        super(GenerativeCustomLoss, self).__init__(loss_function_for_generative_model, name=name, reduction=reduction)\n",
    "\n",
    "class BoxCustomLoss(LossFunctionWrapper):\n",
    "    def __init__(self,\n",
    "        reduction=losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE,\n",
    "        name='generative_custom_loss'):\n",
    "        \n",
    "        def box_loss(y_true, y_pred):\n",
    "            map_part = y_pred;\n",
    "            return inblock_loss(map_part[:, :, 0:2]) + inblock_loss(map_part[:, :, 4:6])\n",
    "        \n",
    "        super(BoxCustomLoss, self).__init__(box_loss, name=name, reduction=reduction)\n",
    "\n",
    "class AlwaysZeroCustomLoss(LossFunctionWrapper): # why does TF not include this! this is very important in certain situations\n",
    "    def __init__(self,\n",
    "        reduction=losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE,\n",
    "        name='generative_custom_loss'):\n",
    "        \n",
    "        def alw_zero(y_true, y_pred):\n",
    "            return tf.convert_to_tensor(0, dtype=tf.float32);\n",
    "        \n",
    "        super(AlwaysZeroCustomLoss, self).__init__(alw_zero, name=name, reduction=reduction)\n",
    "        \n",
    "        \n",
    "class KerasCustomMappingLayer(keras.layers.Layer):\n",
    "    def __init__(self, extvar, output_shape=(special_train_data.shape[1], special_train_data.shape[2]), *args, **kwargs):\n",
    "        self.extvar = extvar\n",
    "        self._output_shape = output_shape\n",
    "        self.extvar_begin = tf.Variable(tf.convert_to_tensor(extvar[\"begin\"], dtype=tf.int32), trainable=False)\n",
    "        self.extvar_lmul =  tf.Variable(tf.convert_to_tensor([extvar[\"length_multiplier\"]], dtype=tf.float32), trainable=False)\n",
    "        self.extvar_nfse =  tf.Variable(tf.convert_to_tensor(extvar[\"next_from_slider_end\"], dtype=tf.bool), trainable=False)\n",
    "        self.note_group_size = GAN_PARAMS[\"note_group_size\"];\n",
    "        \n",
    "        self.extvar_spos =  tf.Variable(tf.cast(tf.zeros((2, )), tf.float32), trainable=False)\n",
    "        self.extvar_rel =   tf.Variable(tf.cast(tf.zeros((7, self.note_group_size)), tf.float32), trainable=False)\n",
    "        \n",
    "        super(KerasCustomMappingLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def build(self, input_shape): # since this is a static layer, no building is required\n",
    "        pass\n",
    "    \n",
    "    def set_extvar(self, extvar):\n",
    "        self.extvar = extvar;\n",
    "        \n",
    "        # Populate extvar with the rel variable (this will modify the input extvar)\n",
    "        begin_offset = extvar[\"begin\"];\n",
    "        self.extvar[\"relevant_tensors\"] = {\n",
    "            \"is_slider\"       : tf.convert_to_tensor(is_slider      [begin_offset : begin_offset + self.note_group_size], dtype=tf.bool),\n",
    "            \"slider_lengths\"  : tf.convert_to_tensor(slider_lengths [begin_offset : begin_offset + self.note_group_size], dtype=tf.float32),\n",
    "            \"slider_types\"    : tf.convert_to_tensor(slider_types   [begin_offset : begin_offset + self.note_group_size], dtype=tf.float32),\n",
    "            \"slider_cos_each\" : tf.convert_to_tensor(slider_cos_each[begin_offset : begin_offset + self.note_group_size], dtype=tf.float32),\n",
    "            \"slider_sin_each\" : tf.convert_to_tensor(slider_sin_each[begin_offset : begin_offset + self.note_group_size], dtype=tf.float32),\n",
    "            \"note_distances\" :  tf.convert_to_tensor(note_distances [begin_offset : begin_offset + self.note_group_size], dtype=tf.float32),\n",
    "            \"note_angles\" :     tf.convert_to_tensor(note_angles    [begin_offset : begin_offset + self.note_group_size], dtype=tf.float32)\n",
    "        };\n",
    "        \n",
    "        # Continue\n",
    "        self.extvar_begin.assign(extvar[\"begin\"])\n",
    "        self.extvar_spos.assign(extvar[\"start_pos\"])\n",
    "        self.extvar_lmul.assign([extvar[\"length_multiplier\"]])\n",
    "        self.extvar_nfse.assign(extvar[\"next_from_slider_end\"])\n",
    "        self.extvar_rel.assign(tf.convert_to_tensor([\n",
    "            is_slider      [begin_offset : begin_offset + self.note_group_size],\n",
    "            slider_lengths [begin_offset : begin_offset + self.note_group_size],\n",
    "            slider_types   [begin_offset : begin_offset + self.note_group_size],\n",
    "            slider_cos_each[begin_offset : begin_offset + self.note_group_size],\n",
    "            slider_sin_each[begin_offset : begin_offset + self.note_group_size],\n",
    "            note_distances [begin_offset : begin_offset + self.note_group_size],\n",
    "            note_angles    [begin_offset : begin_offset + self.note_group_size]\n",
    "        ], dtype=tf.float32))\n",
    "\n",
    "    # Call method will sometimes get used in graph mode,\n",
    "    # training will get turned into a tensor\n",
    "#     @tf.function\n",
    "    def call(self, inputs, training=None):\n",
    "        mapvars = inputs;\n",
    "        start_pos = self.extvar_spos\n",
    "        rel = self.extvar_rel\n",
    "        extvar = {\n",
    "            \"begin\" : self.extvar_begin,\n",
    "            # \"start_pos\" : self.extvar_start_pos,\n",
    "            \"start_pos\" : tf.cast(start_pos, tf.float32),\n",
    "            \"length_multiplier\" : self.extvar_lmul,\n",
    "            \"next_from_slider_end\" : self.extvar_nfse,\n",
    "            # \"relevant_tensors\" : self.extvar_rel\n",
    "            \"relevant_tensors\" : {\n",
    "                \"is_slider\"       : tf.cast(rel[0], tf.bool),\n",
    "                \"slider_lengths\"  : tf.cast(rel[1], tf.float32),\n",
    "                \"slider_types\"    : tf.cast(rel[2], tf.float32),\n",
    "                \"slider_cos_each\" : tf.cast(rel[3], tf.float32),\n",
    "                \"slider_sin_each\" : tf.cast(rel[4], tf.float32),\n",
    "                \"note_distances\"  : tf.cast(rel[5], tf.float32),\n",
    "                \"note_angles\"     : tf.cast(rel[6], tf.float32)\n",
    "            }\n",
    "        }\n",
    "        result = construct_map_with_sliders(mapvars, extvar=extvar);\n",
    "        return result;\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RwWPaJryD2aN"
   },
   "source": [
    "Now we can train the model!\n",
    "\n",
    "This will take some time. It splits map into groups of 10 (10 by default), and trains GAN to play with the flow of maps in the dataset. It will train for floor(note_count / 10) groups, for example, if you have 820 then it needs to train 82 groups.\n",
    "\n",
    "Some notes are converted to sliders, so it has less than the predicted count in #6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XdfkR223D9dW",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of groups: 25\n",
      "Group 0, Epoch 1: G loss: 0.2912032646792276 vs. C loss: 0.20495415065023634\n",
      "Group 0, Epoch 2: G loss: 0.29235615985734115 vs. C loss: 0.14479984508620367\n",
      "Group 0, Epoch 3: G loss: 0.5332135472978864 vs. C loss: 0.1283768688639005\n",
      "Group 0, Epoch 4: G loss: 0.39552748671599797 vs. C loss: 0.19969529575771758\n",
      "Group 0, Epoch 5: G loss: 0.10808064692786763 vs. C loss: 0.20898514489332834\n",
      "Group 0, Epoch 6: G loss: 0.13376436339957373 vs. C loss: 0.1804100970427195\n",
      "Group 0, Epoch 7: G loss: 0.3554777451923915 vs. C loss: 0.10999229550361633\n",
      "Group 0, Epoch 8: G loss: 0.6242116144725255 vs. C loss: 0.0751733448770311\n",
      "Group 0, Epoch 9: G loss: 0.37789844827992575 vs. C loss: 0.13943456444475386\n",
      "Group 0, Epoch 10: G loss: 0.5235733445201601 vs. C loss: 0.23408202495839858\n",
      "Group 0, Epoch 11: G loss: 0.04460947918040412 vs. C loss: 0.20925895704163444\n",
      "Group 1, Epoch 1: G loss: 0.30240684066499984 vs. C loss: 0.25732604993714225\n",
      "Group 1, Epoch 2: G loss: 0.24316672555037908 vs. C loss: 0.1822841895951165\n",
      "Group 1, Epoch 3: G loss: 0.19900359213352203 vs. C loss: 0.18197489115926957\n",
      "Group 1, Epoch 4: G loss: 0.28786887867110117 vs. C loss: 0.14114555882083044\n",
      "Group 1, Epoch 5: G loss: 0.4576483249664306 vs. C loss: 0.12214663459195031\n",
      "Group 1, Epoch 6: G loss: 0.5612037794930596 vs. C loss: 0.04457366466522217\n",
      "Group 1, Epoch 7: G loss: 0.6643532037734985 vs. C loss: 0.09299684522880448\n",
      "Group 1, Epoch 8: G loss: 0.6974140337535312 vs. C loss: 0.08464005962014198\n",
      "Group 2, Epoch 1: G loss: 0.28663094895226615 vs. C loss: 0.27161456478966606\n",
      "Group 2, Epoch 2: G loss: 0.2600897129092898 vs. C loss: 0.18506555838717356\n",
      "Group 2, Epoch 3: G loss: 0.2067422585827964 vs. C loss: 0.1478840642505222\n",
      "Group 2, Epoch 4: G loss: 0.4626487357275826 vs. C loss: 0.09113912863863839\n",
      "Group 2, Epoch 5: G loss: 0.7675674472536358 vs. C loss: 0.03524824045598507\n",
      "Group 2, Epoch 6: G loss: 0.41065257745129724 vs. C loss: 0.18226783184541598\n",
      "Group 2, Epoch 7: G loss: 0.15278224838631496 vs. C loss: 0.22230774660905203\n",
      "Group 3, Epoch 1: G loss: 0.2959431567362377 vs. C loss: 0.26469341251585216\n",
      "Group 3, Epoch 2: G loss: 0.20111585302012308 vs. C loss: 0.164261089430915\n",
      "Group 3, Epoch 3: G loss: 0.20268756108624597 vs. C loss: 0.15416334403885734\n",
      "Group 3, Epoch 4: G loss: 0.3927450358867646 vs. C loss: 0.10773507588439517\n",
      "Group 3, Epoch 5: G loss: 0.46319265450750086 vs. C loss: 0.1894147644440333\n",
      "Group 3, Epoch 6: G loss: 0.11975017935037613 vs. C loss: 0.16785667091608047\n",
      "Group 4, Epoch 1: G loss: 0.34089728849274764 vs. C loss: 0.2761172453562419\n",
      "Group 4, Epoch 2: G loss: 0.2789810491459711 vs. C loss: 0.19126620723141563\n",
      "Group 4, Epoch 3: G loss: 0.19868516751698087 vs. C loss: 0.15366522636678484\n",
      "Group 4, Epoch 4: G loss: 0.384572639635631 vs. C loss: 0.12330006642474067\n",
      "Group 4, Epoch 5: G loss: 0.6041714770453316 vs. C loss: 0.08793916925787926\n",
      "Group 4, Epoch 6: G loss: 0.27886479773691725 vs. C loss: 0.20266798304186925\n",
      "Group 4, Epoch 7: G loss: 0.07439484958137785 vs. C loss: 0.19920536296235192\n",
      "Group 4, Epoch 8: G loss: 0.11277204517807281 vs. C loss: 0.16809934791591433\n",
      "Group 4, Epoch 9: G loss: 0.39482580785240445 vs. C loss: 0.1926728437344233\n",
      "Group 4, Epoch 10: G loss: 0.19000894874334337 vs. C loss: 0.1596884404619535\n",
      "Group 4, Epoch 11: G loss: 0.5825179696083069 vs. C loss: 0.06452606473531988\n",
      "Group 4, Epoch 12: G loss: 0.7485950350761412 vs. C loss: 0.01559314400785499\n",
      "Group 4, Epoch 13: G loss: 0.8984189425195968 vs. C loss: 0.0036121755191642377\n",
      "Group 4, Epoch 14: G loss: 0.9744441373007638 vs. C loss: 0.0006211083091329783\n",
      "Group 4, Epoch 15: G loss: 1.0023391587393624 vs. C loss: 0.0010386492649558932\n",
      "Group 4, Epoch 16: G loss: 1.007479981013707 vs. C loss: 0.00022044019068643035\n",
      "Group 4, Epoch 17: G loss: 0.9950840813773019 vs. C loss: 0.0007515910902940151\n",
      "Group 4, Epoch 18: G loss: 0.9859629256384712 vs. C loss: 0.0010526433179620653\n",
      "Group 4, Epoch 19: G loss: 0.982816091605595 vs. C loss: 0.00031277837115339935\n",
      "Group 4, Epoch 20: G loss: 0.9930934344019208 vs. C loss: 0.00014645965388303416\n",
      "Group 4, Epoch 21: G loss: 1.0034116744995116 vs. C loss: 8.090466346604645e-05\n",
      "Group 4, Epoch 22: G loss: 1.0078361749649047 vs. C loss: 0.00014773683182688223\n",
      "Group 4, Epoch 23: G loss: 1.0098190239497593 vs. C loss: 6.461153275773137e-05\n",
      "Group 4, Epoch 24: G loss: 1.0107617752892628 vs. C loss: 4.0165189501648354e-05\n",
      "Group 4, Epoch 25: G loss: 1.0113436358315604 vs. C loss: 3.5702812561390194e-05\n",
      "Group 5, Epoch 1: G loss: 0.30320989489555356 vs. C loss: 0.2497577153974109\n",
      "Group 5, Epoch 2: G loss: 0.22073322321687422 vs. C loss: 0.17578906317551932\n",
      "Group 5, Epoch 3: G loss: 0.21956391760281155 vs. C loss: 0.16274107661512163\n",
      "Group 5, Epoch 4: G loss: 0.46482546159199306 vs. C loss: 0.1128434348437521\n",
      "Group 5, Epoch 5: G loss: 0.5172749093600683 vs. C loss: 0.11680812885363896\n",
      "Group 5, Epoch 6: G loss: 0.2765313187880175 vs. C loss: 0.21244706958532333\n",
      "Group 5, Epoch 7: G loss: 0.22422111545290266 vs. C loss: 0.14628447012768853\n",
      "Group 5, Epoch 8: G loss: 0.3942746038947787 vs. C loss: 0.14224798563453886\n",
      "Group 5, Epoch 9: G loss: 0.6411779011998858 vs. C loss: 0.028345647112776835\n",
      "Group 5, Epoch 10: G loss: 0.8267055511474609 vs. C loss: 0.010901198205020694\n",
      "Group 5, Epoch 11: G loss: 0.9363276685987199 vs. C loss: 0.006763805066131883\n",
      "Group 5, Epoch 12: G loss: 0.49412676564284735 vs. C loss: 0.04903111587433765\n",
      "Group 5, Epoch 13: G loss: 0.9759603108678546 vs. C loss: 0.005656487360182736\n",
      "Group 6, Epoch 1: G loss: 0.2707828095981053 vs. C loss: 0.26239854759640163\n",
      "Group 6, Epoch 2: G loss: 0.19375531332833426 vs. C loss: 0.1840298225482305\n",
      "Group 6, Epoch 3: G loss: 0.13343260692698616 vs. C loss: 0.18224510302146277\n",
      "Group 6, Epoch 4: G loss: 0.2311004719563893 vs. C loss: 0.15565618375937143\n",
      "Group 6, Epoch 5: G loss: 0.3660915574857167 vs. C loss: 0.14993336962329015\n",
      "Group 6, Epoch 6: G loss: 0.4440159440040588 vs. C loss: 0.08429961734347872\n",
      "Group 7, Epoch 1: G loss: 0.32364520686013354 vs. C loss: 0.2505753950940238\n",
      "Group 7, Epoch 2: G loss: 0.30159630605152676 vs. C loss: 0.19227094782723322\n",
      "Group 7, Epoch 3: G loss: 0.16065121165343693 vs. C loss: 0.15982219411267173\n",
      "Group 7, Epoch 4: G loss: 0.2687044518334525 vs. C loss: 0.12585517598523033\n",
      "Group 7, Epoch 5: G loss: 0.4898051291704178 vs. C loss: 0.17607231769296858\n",
      "Group 7, Epoch 6: G loss: 0.20821371908698763 vs. C loss: 0.17967337121566138\n",
      "Group 8, Epoch 1: G loss: 0.302985098532268 vs. C loss: 0.2723396023114522\n",
      "Group 8, Epoch 2: G loss: 0.2652870765754155 vs. C loss: 0.17361652851104736\n",
      "Group 8, Epoch 3: G loss: 0.18445689976215363 vs. C loss: 0.16000046167108747\n",
      "Group 8, Epoch 4: G loss: 0.2974272549152374 vs. C loss: 0.15579994850688514\n",
      "Group 8, Epoch 5: G loss: 0.35731765372412544 vs. C loss: 0.1813600030210283\n",
      "Group 8, Epoch 6: G loss: 0.29129468032291955 vs. C loss: 0.12410881204737557\n",
      "Group 9, Epoch 1: G loss: 0.3079947829246521 vs. C loss: 0.2602035188012653\n",
      "Group 9, Epoch 2: G loss: 0.3252680437905448 vs. C loss: 0.1658532718817393\n",
      "Group 9, Epoch 3: G loss: 0.20731351971626277 vs. C loss: 0.2017309276594056\n",
      "Group 9, Epoch 4: G loss: 0.06491521650127001 vs. C loss: 0.19708950486448076\n",
      "Group 9, Epoch 5: G loss: 0.06524742916226388 vs. C loss: 0.18329032096597886\n",
      "Group 9, Epoch 6: G loss: 0.19497574908392767 vs. C loss: 0.16421143876181707\n",
      "Group 10, Epoch 1: G loss: 0.35708707485880176 vs. C loss: 0.25263762805196976\n",
      "Group 10, Epoch 2: G loss: 0.23583872616291046 vs. C loss: 0.1864700714747111\n",
      "Group 10, Epoch 3: G loss: 0.1476245173386165 vs. C loss: 0.1682405157221688\n",
      "Group 10, Epoch 4: G loss: 0.2780507556029729 vs. C loss: 0.14197035630544028\n",
      "Group 10, Epoch 5: G loss: 0.33475127645901276 vs. C loss: 0.1344448063108656\n",
      "Group 10, Epoch 6: G loss: 0.49629589915275574 vs. C loss: 0.08661134623818928\n",
      "Group 10, Epoch 7: G loss: 0.49247747446809503 vs. C loss: 0.15617807788981333\n",
      "Group 11, Epoch 1: G loss: 0.3023509238447462 vs. C loss: 0.27130909760793054\n",
      "Group 11, Epoch 2: G loss: 0.3080556418214525 vs. C loss: 0.19110809100998774\n",
      "Group 11, Epoch 3: G loss: 0.1960555387394769 vs. C loss: 0.18629132625129485\n",
      "Group 11, Epoch 4: G loss: 0.17246452697685788 vs. C loss: 0.14020900925000507\n",
      "Group 11, Epoch 5: G loss: 0.5163525308881488 vs. C loss: 0.0910060786538654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 11, Epoch 6: G loss: 0.6004330737250191 vs. C loss: 0.04049373262872298\n",
      "Group 11, Epoch 7: G loss: 0.8253981505121504 vs. C loss: 0.01364480114231507\n",
      "Group 11, Epoch 8: G loss: 0.8754258411271232 vs. C loss: 0.006191781372763216\n",
      "Group 11, Epoch 9: G loss: 0.948059288093022 vs. C loss: 0.0023839352717105714\n",
      "Group 11, Epoch 10: G loss: 0.9695728165762765 vs. C loss: 0.015220059786871491\n",
      "Group 11, Epoch 11: G loss: 0.4590775075235537 vs. C loss: 0.20850967698627051\n",
      "Group 11, Epoch 12: G loss: 0.011995197060917106 vs. C loss: 0.2081604947646459\n",
      "Group 11, Epoch 13: G loss: 0.00577495155323829 vs. C loss: 0.20822672545909882\n",
      "Group 12, Epoch 1: G loss: 0.29278588465281896 vs. C loss: 0.2776596330934101\n",
      "Group 12, Epoch 2: G loss: 0.2341607608965465 vs. C loss: 0.17253068172269395\n",
      "Group 12, Epoch 3: G loss: 0.22747367961066112 vs. C loss: 0.12984014633629057\n",
      "Group 12, Epoch 4: G loss: 0.514714160987309 vs. C loss: 0.08549887024694018\n",
      "Group 12, Epoch 5: G loss: 0.6520845021520342 vs. C loss: 0.08446087853776085\n",
      "Group 12, Epoch 6: G loss: 0.5419987218720572 vs. C loss: 0.12125632911920549\n",
      "Group 13, Epoch 1: G loss: 0.3488397325788225 vs. C loss: 0.24529859754774305\n",
      "Group 13, Epoch 2: G loss: 0.24362519596304208 vs. C loss: 0.1996294152405527\n",
      "Group 13, Epoch 3: G loss: 0.12664277596133097 vs. C loss: 0.16097033354971144\n",
      "Group 13, Epoch 4: G loss: 0.4528211295604706 vs. C loss: 0.11848866773976219\n",
      "Group 13, Epoch 5: G loss: 0.47446460212979996 vs. C loss: 0.06458743123544587\n",
      "Group 13, Epoch 6: G loss: 0.8724377785410199 vs. C loss: 0.016150465338594384\n",
      "Group 14, Epoch 1: G loss: 0.31393870285579134 vs. C loss: 0.24841833445760939\n",
      "Group 14, Epoch 2: G loss: 0.2222678691148758 vs. C loss: 0.1779885556962755\n",
      "Group 14, Epoch 3: G loss: 0.23819530265671868 vs. C loss: 0.12774931225511763\n",
      "Group 14, Epoch 4: G loss: 0.3726644888520241 vs. C loss: 0.23223631083965302\n",
      "Group 14, Epoch 5: G loss: 0.02841891795396805 vs. C loss: 0.20316077437665725\n",
      "Group 14, Epoch 6: G loss: 0.021420021301933694 vs. C loss: 0.20047748419973588\n",
      "Group 15, Epoch 1: G loss: 0.31436172127723694 vs. C loss: 0.27092886964480084\n",
      "Group 15, Epoch 2: G loss: 0.2646608786923545 vs. C loss: 0.18080910874737632\n",
      "Group 15, Epoch 3: G loss: 0.2000481307506561 vs. C loss: 0.1445867915948232\n",
      "Group 15, Epoch 4: G loss: 0.3961706101894379 vs. C loss: 0.10471831096543206\n",
      "Group 15, Epoch 5: G loss: 0.6241049715450833 vs. C loss: 0.04980861436989572\n",
      "Group 15, Epoch 6: G loss: 0.7351329582078117 vs. C loss: 0.02830934534884161\n",
      "Group 16, Epoch 1: G loss: 0.30711026021412435 vs. C loss: 0.23973472664753595\n",
      "Group 16, Epoch 2: G loss: 0.2589542239904404 vs. C loss: 0.16701282643609575\n",
      "Group 16, Epoch 3: G loss: 0.3221954115799495 vs. C loss: 0.11713444110420014\n",
      "Group 16, Epoch 4: G loss: 0.3698442880596433 vs. C loss: 0.22300924526320565\n",
      "Group 16, Epoch 5: G loss: 0.07679144857185227 vs. C loss: 0.2029483781920539\n",
      "Group 16, Epoch 6: G loss: 0.12438625246286393 vs. C loss: 0.12030538419882457\n",
      "Group 17, Epoch 1: G loss: 0.30227087949003495 vs. C loss: 0.275495277510749\n",
      "Group 17, Epoch 2: G loss: 0.2739086491721017 vs. C loss: 0.16606762011845908\n",
      "Group 17, Epoch 3: G loss: 0.268721029162407 vs. C loss: 0.14759266790416506\n",
      "Group 17, Epoch 4: G loss: 0.29290231892040797 vs. C loss: 0.20372445219092897\n",
      "Group 17, Epoch 5: G loss: 0.2091770992747375 vs. C loss: 0.22452421817514634\n",
      "Group 17, Epoch 6: G loss: 0.06594450920820237 vs. C loss: 0.1882255102197329\n",
      "Group 18, Epoch 1: G loss: 0.34051025765282766 vs. C loss: 0.2660869492424859\n",
      "Group 18, Epoch 2: G loss: 0.2889144957065582 vs. C loss: 0.1570510814587275\n",
      "Group 18, Epoch 3: G loss: 0.3266193960394178 vs. C loss: 0.12934026287661657\n",
      "Group 18, Epoch 4: G loss: 0.33399508297443387 vs. C loss: 0.17711391051610312\n",
      "Group 18, Epoch 5: G loss: 0.2043718308210373 vs. C loss: 0.21189857936567733\n",
      "Group 18, Epoch 6: G loss: 0.2002606489828655 vs. C loss: 0.20839775602022806\n",
      "Group 18, Epoch 7: G loss: 0.07890055520193917 vs. C loss: 0.18772847081224123\n",
      "Group 18, Epoch 8: G loss: 0.1284933377589498 vs. C loss: 0.148626286122534\n",
      "Group 19, Epoch 1: G loss: 0.2873524797814233 vs. C loss: 0.27646097540855413\n",
      "Group 19, Epoch 2: G loss: 0.28293915050370355 vs. C loss: 0.1679727352327771\n",
      "Group 19, Epoch 3: G loss: 0.31839457367147717 vs. C loss: 0.15242581648959055\n",
      "Group 19, Epoch 4: G loss: 0.5545112473624093 vs. C loss: 0.11134540372424656\n",
      "Group 19, Epoch 5: G loss: 0.653727332183293 vs. C loss: 0.09964665232433212\n",
      "Group 19, Epoch 6: G loss: 0.27477724573441914 vs. C loss: 0.20908100571897295\n",
      "Group 19, Epoch 7: G loss: 0.0464258679321834 vs. C loss: 0.20786023139953613\n",
      "Group 19, Epoch 8: G loss: 0.03731389194726944 vs. C loss: 0.20795252753628626\n",
      "Group 19, Epoch 9: G loss: 0.03345164677926472 vs. C loss: 0.2079649493098259\n",
      "Group 19, Epoch 10: G loss: 0.03181599644677979 vs. C loss: 0.20787452906370163\n",
      "Group 19, Epoch 11: G loss: 0.03167109627808844 vs. C loss: 0.20780249146951568\n",
      "Group 19, Epoch 12: G loss: 0.031071891635656358 vs. C loss: 0.20768527686595917\n",
      "Group 19, Epoch 13: G loss: 0.03138804653925555 vs. C loss: 0.20740784787469438\n",
      "Group 19, Epoch 14: G loss: 0.03221776964409011 vs. C loss: 0.2069113850593567\n",
      "Group 19, Epoch 15: G loss: 0.03473475873470306 vs. C loss: 0.2056062254640791\n",
      "Group 19, Epoch 16: G loss: 0.04202713529978479 vs. C loss: 0.20071257156733838\n",
      "Group 19, Epoch 17: G loss: 0.08170037397316522 vs. C loss: 0.16743182308144042\n",
      "Group 19, Epoch 18: G loss: 0.3483753238405501 vs. C loss: 0.0891917310655117\n",
      "Group 19, Epoch 19: G loss: 0.8096746257373264 vs. C loss: 0.019126264274948172\n",
      "Group 19, Epoch 20: G loss: 0.9626036473682948 vs. C loss: 0.0021266439548020973\n",
      "Group 19, Epoch 21: G loss: 1.011048800604684 vs. C loss: 0.004680886463676062\n",
      "Group 19, Epoch 22: G loss: 0.7227721316473824 vs. C loss: 0.04928715508948597\n",
      "Group 19, Epoch 23: G loss: 1.0574423381260465 vs. C loss: 0.06469926833071643\n",
      "Group 19, Epoch 24: G loss: 0.5417951494455339 vs. C loss: 0.2064566190044085\n",
      "Group 19, Epoch 25: G loss: 0.045341185161045613 vs. C loss: 0.20800613198015425\n",
      "Group 20, Epoch 1: G loss: 0.318864358322961 vs. C loss: 0.2620616952578227\n",
      "Group 20, Epoch 2: G loss: 0.19368430546351842 vs. C loss: 0.19042224023077226\n",
      "Group 20, Epoch 3: G loss: 0.1707161090203694 vs. C loss: 0.13957804772588941\n",
      "Group 20, Epoch 4: G loss: 0.46064911825316296 vs. C loss: 0.13808308127853605\n",
      "Group 20, Epoch 5: G loss: 0.4243671636496272 vs. C loss: 0.22220336397488913\n",
      "Group 20, Epoch 6: G loss: 0.03551439781274114 vs. C loss: 0.20808012813277957\n",
      "Group 20, Epoch 7: G loss: 0.021601919350879534 vs. C loss: 0.20757595118549135\n",
      "Group 20, Epoch 8: G loss: 0.01882827931216785 vs. C loss: 0.2076427853769726\n",
      "Group 20, Epoch 9: G loss: 0.01853620127907821 vs. C loss: 0.2075996614164776\n",
      "Group 20, Epoch 10: G loss: 0.01869813853076526 vs. C loss: 0.2074885881609387\n",
      "Group 20, Epoch 11: G loss: 0.01907421153570924 vs. C loss: 0.20729105174541473\n",
      "Group 20, Epoch 12: G loss: 0.019785022682377272 vs. C loss: 0.2069214185078939\n",
      "Group 20, Epoch 13: G loss: 0.021226086627159798 vs. C loss: 0.20619274510277644\n",
      "Group 20, Epoch 14: G loss: 0.02435620629361698 vs. C loss: 0.20435762777924538\n",
      "Group 20, Epoch 15: G loss: 0.03336389777915818 vs. C loss: 0.1972088317076365\n",
      "Group 20, Epoch 16: G loss: 0.084942646750382 vs. C loss: 0.16125757164425317\n",
      "Group 20, Epoch 17: G loss: 0.44029126507895333 vs. C loss: 0.13011482606331506\n",
      "Group 20, Epoch 18: G loss: 0.3290628235254968 vs. C loss: 0.19515909088982475\n",
      "Group 21, Epoch 1: G loss: 0.3115315582071032 vs. C loss: 0.25636425779925454\n",
      "Group 21, Epoch 2: G loss: 0.28428091406822203 vs. C loss: 0.16127344552013612\n",
      "Group 21, Epoch 3: G loss: 0.4005718818732671 vs. C loss: 0.08747119539313847\n",
      "Group 21, Epoch 4: G loss: 0.590161704165595 vs. C loss: 0.14237770355410048\n",
      "Group 21, Epoch 5: G loss: 0.6898924112319946 vs. C loss: 0.03699074871838093\n",
      "Group 21, Epoch 6: G loss: 0.8514995421682087 vs. C loss: 0.01798152333746354\n",
      "Group 21, Epoch 7: G loss: 0.8488732082503182 vs. C loss: 0.002571453624922368\n",
      "Group 21, Epoch 8: G loss: 0.9752073901040214 vs. C loss: 0.0006240397229299155\n",
      "Group 21, Epoch 9: G loss: 0.932645961216518 vs. C loss: 0.0023023808446143647\n",
      "Group 21, Epoch 10: G loss: 0.9990128789629255 vs. C loss: 0.00143795958138071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 21, Epoch 11: G loss: 0.9977229237556458 vs. C loss: 0.00018504700468232235\n",
      "Group 21, Epoch 12: G loss: 0.9903156263487679 vs. C loss: 0.00023405941060951186\n",
      "Group 21, Epoch 13: G loss: 0.992565906047821 vs. C loss: 0.0006972665885566836\n",
      "Group 21, Epoch 14: G loss: 0.9761302658489771 vs. C loss: 0.0017046677142692108\n",
      "Group 21, Epoch 15: G loss: 0.9901963710784912 vs. C loss: 0.000222900490270048\n",
      "Group 21, Epoch 16: G loss: 1.0083477394921438 vs. C loss: 0.00031847600055496313\n",
      "Group 21, Epoch 17: G loss: 1.0067687000547136 vs. C loss: 0.00022857464743234837\n",
      "Group 21, Epoch 18: G loss: 1.0038826738085067 vs. C loss: 0.00010247744365996266\n",
      "Group 21, Epoch 19: G loss: 1.0009950058800834 vs. C loss: 0.00011226311390702096\n",
      "Group 21, Epoch 20: G loss: 1.002652212551662 vs. C loss: 8.983097035929355e-05\n",
      "Group 21, Epoch 21: G loss: 1.005841292653765 vs. C loss: 6.772069051900569e-05\n",
      "Group 21, Epoch 22: G loss: 1.0079049110412597 vs. C loss: 0.00010884487589161533\n",
      "Group 21, Epoch 23: G loss: 1.0088131121226718 vs. C loss: 0.00010679152910597622\n",
      "Group 21, Epoch 24: G loss: 1.009219091279166 vs. C loss: 4.983992160608371e-05\n",
      "Group 21, Epoch 25: G loss: 1.0036939076014926 vs. C loss: 4.731857310010431e-05\n",
      "Group 22, Epoch 1: G loss: 0.3111719101667404 vs. C loss: 0.26078120205137467\n",
      "Group 22, Epoch 2: G loss: 0.20180520415306089 vs. C loss: 0.19105937249130675\n",
      "Group 22, Epoch 3: G loss: 0.12488744109869003 vs. C loss: 0.15692747715446684\n",
      "Group 22, Epoch 4: G loss: 0.41518454636846275 vs. C loss: 0.1184851030508677\n",
      "Group 22, Epoch 5: G loss: 0.3783862556729998 vs. C loss: 0.20583254761166045\n",
      "Group 22, Epoch 6: G loss: 0.08801434614828654 vs. C loss: 0.18484095484018326\n",
      "Group 22, Epoch 7: G loss: 0.17368695118597577 vs. C loss: 0.183543860912323\n",
      "Group 22, Epoch 8: G loss: 0.6060257962771824 vs. C loss: 0.06954926749070485\n",
      "Group 22, Epoch 9: G loss: 0.6470718545573099 vs. C loss: 0.055082522539628885\n",
      "Group 22, Epoch 10: G loss: 0.929624720982143 vs. C loss: 0.007592989907910426\n",
      "Group 23, Epoch 1: G loss: 0.3066131217139108 vs. C loss: 0.24704274535179138\n",
      "Group 23, Epoch 2: G loss: 0.2421020988907133 vs. C loss: 0.1741417133145862\n",
      "Group 23, Epoch 3: G loss: 0.15784870811871118 vs. C loss: 0.192205172446039\n",
      "Group 23, Epoch 4: G loss: 0.16720829989228927 vs. C loss: 0.19089071287049186\n",
      "Group 23, Epoch 5: G loss: 0.36767846473625726 vs. C loss: 0.15236005187034607\n",
      "Group 23, Epoch 6: G loss: 0.562959464958736 vs. C loss: 0.06889883470204142\n",
      "Group 23, Epoch 7: G loss: 0.7060564994812013 vs. C loss: 0.04878750091625584\n",
      "Group 23, Epoch 8: G loss: 0.8611305832862852 vs. C loss: 0.07238860324852996\n",
      "Group 23, Epoch 9: G loss: 0.8657302805355618 vs. C loss: 0.013991475674427219\n",
      "Group 23, Epoch 10: G loss: 0.81570134971823 vs. C loss: 0.05811033227170508\n",
      "Group 23, Epoch 11: G loss: 1.0666993243353706 vs. C loss: 0.04150777569803823\n",
      "Group 23, Epoch 12: G loss: 1.0282146521977018 vs. C loss: 0.003275641835191184\n",
      "Group 23, Epoch 13: G loss: 0.21591706701687402 vs. C loss: 0.20816413892640007\n",
      "Group 23, Epoch 14: G loss: 0.07522802501916885 vs. C loss: 0.20800649788644576\n",
      "Group 23, Epoch 15: G loss: 0.051760896508182795 vs. C loss: 0.20734407402594315\n",
      "Group 23, Epoch 16: G loss: 0.05063385750566211 vs. C loss: 0.18621730473306444\n",
      "Group 23, Epoch 17: G loss: 0.9807261228561401 vs. C loss: 0.04900896973494027\n",
      "Group 23, Epoch 18: G loss: 1.0283914804458618 vs. C loss: 0.02363670984697011\n",
      "Group 23, Epoch 19: G loss: 0.9217336228915622 vs. C loss: 0.005177723205027481\n",
      "Group 23, Epoch 20: G loss: 0.9423541273389543 vs. C loss: 0.0031063404102395806\n",
      "Group 23, Epoch 21: G loss: 0.9549175671168736 vs. C loss: 0.028744213220003683\n",
      "Group 23, Epoch 22: G loss: 0.856474588598524 vs. C loss: 0.22510452071825662\n",
      "Group 23, Epoch 23: G loss: 0.06733175505484854 vs. C loss: 0.24564674662219152\n",
      "Group 23, Epoch 24: G loss: 0.06117720497506006 vs. C loss: 0.20956920004553262\n",
      "Group 24, Epoch 1: G loss: 0.2939794919320515 vs. C loss: 0.2648776918649674\n",
      "Group 24, Epoch 2: G loss: 0.2466578053576606 vs. C loss: 0.16699988808896807\n",
      "Group 24, Epoch 3: G loss: 0.1880412174122674 vs. C loss: 0.152379689945115\n",
      "Group 24, Epoch 4: G loss: 0.4543206104210445 vs. C loss: 0.11423901716868083\n",
      "Group 24, Epoch 5: G loss: 0.5590547808579036 vs. C loss: 0.10755819413397048\n",
      "Group 24, Epoch 6: G loss: 0.39273731453078137 vs. C loss: 0.19869676315122178\n",
      "Group 24, Epoch 7: G loss: 0.2574397248881204 vs. C loss: 0.13882239907979965\n",
      "Group 24, Epoch 8: G loss: 0.6843224252973285 vs. C loss: 0.04367399339874586\n",
      "Group 24, Epoch 9: G loss: 0.9126442466463361 vs. C loss: 0.018679701360977356\n",
      "Group 24, Epoch 10: G loss: 0.8840471318789891 vs. C loss: 0.006453942197064559\n",
      "Group 24, Epoch 11: G loss: 0.9378493598529271 vs. C loss: 0.001065977655040721\n",
      "Group 24, Epoch 12: G loss: 0.8834113938467844 vs. C loss: 0.005663409175920404\n",
      "Group 24, Epoch 13: G loss: 0.9350280267851694 vs. C loss: 0.027699223758342367\n",
      "Group 24, Epoch 14: G loss: 1.0101283686501639 vs. C loss: 0.05333004876350363\n",
      "Group 24, Epoch 15: G loss: 1.0334249087742398 vs. C loss: 0.004674430946276213\n",
      "Group 24, Epoch 16: G loss: 0.798939597606659 vs. C loss: 0.007492484385794442\n",
      "Group 24, Epoch 17: G loss: 0.9449731877871923 vs. C loss: 0.0325134173164972\n",
      "Group 24, Epoch 18: G loss: 0.9532369698796953 vs. C loss: 0.0022837167165966495\n",
      "Group 24, Epoch 19: G loss: 1.02069194316864 vs. C loss: 0.004599937632317758\n",
      "Group 24, Epoch 20: G loss: 0.9672586492129736 vs. C loss: 0.000921934771314328\n",
      "Group 24, Epoch 21: G loss: 1.038906901223319 vs. C loss: 7.83788699562946e-06\n",
      "Group 24, Epoch 22: G loss: 1.0365082604544502 vs. C loss: 0.019054209419765458\n",
      "Group 24, Epoch 23: G loss: 0.9958981565066746 vs. C loss: 0.04444217576201481\n",
      "Group 24, Epoch 24: G loss: 1.0297605752944947 vs. C loss: 0.1647268533706665\n",
      "Group 24, Epoch 25: G loss: 0.8850211313792637 vs. C loss: 0.04710169819494089\n"
     ]
    }
   ],
   "source": [
    "loss_ma = [90, 90, 90];\n",
    "extvar = {\"begin\": 10};\n",
    "\n",
    "def plot_current_map(inputs):\n",
    "    # plot it each epoch\n",
    "    mp = construct_map_with_sliders(inputs, extvar=extvar);\n",
    "    # to make it clearer, add the start pos\n",
    "    npa = np.concatenate([[np.concatenate([extvar[\"start_pos\"] / np.array([512, 384]), [0, 0]])], tf.stack(mp).numpy().squeeze()])\n",
    "    fig, ax = plt.subplots()\n",
    "    x, y = np.transpose(npa)[0:2]\n",
    "    #x, y = np.random.rand(2, 20)\n",
    "    line = MyLine(x, y, mfc='red', ms=12)\n",
    "    line.text.set_color('red')\n",
    "    line.text.set_fontsize(16)\n",
    "    ax.add_line(line)\n",
    "    plt.show()\n",
    "\n",
    "def generative_model(in_params, out_params, loss_func='mse'):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(128, input_shape=(in_params,)),# activation=tf.nn.elu, input_shape=(train_data.shape[1],)),\n",
    "        keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(128, activation=tf.nn.tanh),\n",
    "        keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(out_params, activation=tf.nn.tanh)#,\n",
    "#         keras.layers.Lambda(lambda x: (x+1)/2, output_shape=(out_params,))\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        optimizer = tf.optimizers.Adam(0.002) #Adamoptimizer?\n",
    "    except:\n",
    "        optimizer = tf.train.AdamOptimizer(0.002) #Adamoptimizer?\n",
    "\n",
    "    model.compile(loss=loss_func,\n",
    "                optimizer=optimizer,\n",
    "                metrics=[keras.metrics.mae])\n",
    "    return model\n",
    "\n",
    "def mixed_model(generator, mapping_layer, discriminator, in_params):\n",
    "    note_group_size = GAN_PARAMS[\"note_group_size\"];\n",
    "    inp = keras.layers.Input(shape=(in_params,))\n",
    "    start_pos = keras.layers.Input(shape = (2,))#tf.convert_to_tensor([0, 0], dtype=tf.float32)\n",
    "    rel = keras.layers.Input(shape = (7, note_group_size))#tf.zeros((5, note_group_size), dtype=tf.float32)\n",
    "    interm1 = generator(inp)\n",
    "    interm2 = mapping_layer(interm1)\n",
    "    end = discriminator(interm2)\n",
    "    model = keras.Model(inputs = inp, outputs = [interm1, interm2, end])\n",
    "    \n",
    "    discriminator.trainable = False\n",
    "\n",
    "    try:\n",
    "        optimizer = tf.optimizers.Adam(0.001) #Adamoptimizer?\n",
    "    except:\n",
    "        optimizer = tf.train.AdamOptimizer(0.001) #Adamoptimizer?\n",
    "        \n",
    "    losses = [AlwaysZeroCustomLoss(), BoxCustomLoss(), GenerativeCustomLoss()];\n",
    "\n",
    "    model.compile(loss=losses,\n",
    "                  loss_weights=[1e-8, 1, 1],\n",
    "                optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "def conv_input(inp, extvar):\n",
    "#     Now it only uses single input\n",
    "    return inp;\n",
    "\n",
    "\n",
    "plot_noise = np.random.random((1, GAN_PARAMS[\"g_input_size\"]));\n",
    "\n",
    "# Pre-fit classifier for 1 epoch\n",
    "# history = classifier_model.fit(actual_train_data, actual_train_labels, epochs=1,\n",
    "#                     validation_split=0.2, verbose=0,\n",
    "#                     callbacks=[])\n",
    "\n",
    "# build models first, then train (it is faster in TF 2.0)\n",
    "\n",
    "def make_models():\n",
    "        \n",
    "    extvar[\"begin\"] = 0;\n",
    "    extvar[\"start_pos\"] = [256, 192];\n",
    "    extvar[\"length_multiplier\"] = 1;\n",
    "    extvar[\"next_from_slider_end\"] = GAN_PARAMS[\"next_from_slider_end\"];\n",
    "    \n",
    "    classifier_model = build_classifier_model();\n",
    "    note_group_size = GAN_PARAMS[\"note_group_size\"];\n",
    "    g_input_size = GAN_PARAMS[\"g_input_size\"];\n",
    "    \n",
    "    gmodel = generative_model(g_input_size, note_group_size * 4);\n",
    "    mapping_layer = KerasCustomMappingLayer(extvar);\n",
    "    mmodel = mixed_model(gmodel, mapping_layer, classifier_model, g_input_size);\n",
    "    \n",
    "    default_weights = mmodel.get_weights();\n",
    "    \n",
    "    return gmodel, mapping_layer, classifier_model, mmodel, default_weights;\n",
    "\n",
    "def set_extvar(models, extvar):\n",
    "    gmodel, mapping_layer, classifier_model, mmodel, default_weights = models;\n",
    "    mapping_layer.set_extvar(extvar);\n",
    "    \n",
    "def reset_model_weights(models):\n",
    "    gmodel, mapping_layer, classifier_model, mmodel, default_weights = models;\n",
    "    weights = default_weights;\n",
    "    mmodel.set_weights(weights);\n",
    "\n",
    "# we can train all the classifiers first, onto Epoch X [x = 1~10]\n",
    "# then train the generators to fit to them\n",
    "# to reduce some training time.\n",
    "# but i think it doesn't work too well since it's the generator which is slow...\n",
    "\n",
    "def generate_set(models, begin = 0, start_pos=[256, 192], group_id=-1, length_multiplier=1, plot_map=True):\n",
    "    extvar[\"begin\"] = begin;\n",
    "    extvar[\"start_pos\"] = start_pos;\n",
    "    extvar[\"length_multiplier\"] = length_multiplier;\n",
    "    extvar[\"next_from_slider_end\"] = GAN_PARAMS[\"next_from_slider_end\"];\n",
    "    \n",
    "    note_group_size = GAN_PARAMS[\"note_group_size\"];\n",
    "    max_epoch = GAN_PARAMS[\"max_epoch\"];\n",
    "    good_epoch = GAN_PARAMS[\"good_epoch\"] - 1;\n",
    "    g_multiplier = GAN_PARAMS[\"g_epochs\"];\n",
    "    c_multiplier = GAN_PARAMS[\"c_epochs\"];\n",
    "    g_batch = GAN_PARAMS[\"g_batch\"];\n",
    "    g_input_size = GAN_PARAMS[\"g_input_size\"];\n",
    "    c_true_batch = GAN_PARAMS[\"c_true_batch\"];\n",
    "    c_false_batch = GAN_PARAMS[\"c_false_batch\"];\n",
    "    \n",
    "    reset_model_weights(models);\n",
    "    set_extvar(models, extvar);\n",
    "    gmodel, mapping_layer, classifier_model, mmodel, default_weights = models;\n",
    "    \n",
    "    # see the summaries\n",
    "#     gmodel.summary()\n",
    "#     classifier_model.summary()\n",
    "#     mmodel.summary()\n",
    "\n",
    "    for i in range(max_epoch):\n",
    "        \n",
    "        gnoise = np.random.random((g_batch, g_input_size));\n",
    "        glabel = [np.zeros((g_batch, note_group_size * 4)), np.ones((g_batch,)), np.ones((g_batch,))]\n",
    "        ginput = conv_input(gnoise, extvar);\n",
    "        \n",
    "        # fit mmodel instead of gmodel\n",
    "        history = mmodel.fit(ginput, glabel, epochs=g_multiplier,\n",
    "                            validation_split=0.2, verbose=0,\n",
    "                            callbacks=[])\n",
    "        \n",
    "        pred_noise = np.random.random((c_false_batch, g_input_size));\n",
    "        pred_input = conv_input(pred_noise, extvar);\n",
    "        predicted_maps_data, predicted_maps_mapped, _predclass = mmodel.predict(pred_input);\n",
    "        new_false_maps = predicted_maps_mapped;\n",
    "        new_false_labels = np.zeros(c_false_batch);\n",
    "        \n",
    "\n",
    "        rn = np.random.randint(0, special_train_data.shape[0], (c_true_batch,))\n",
    "        actual_train_data = np.concatenate((new_false_maps, special_train_data[rn]), axis=0); #special_false_data[st:se], \n",
    "        actual_train_labels = np.concatenate((new_false_labels, special_train_labels[rn]), axis=0); #special_false_labels[st:se], \n",
    "        \n",
    "    \n",
    "        history2 = classifier_model.fit(actual_train_data, actual_train_labels, epochs=c_multiplier,\n",
    "                            validation_split=0.2, verbose=0,\n",
    "                            callbacks=[])\n",
    "        \n",
    "        # calculate the losses\n",
    "        g_loss = np.mean(history.history['loss']);\n",
    "        c_loss = np.mean(history2.history['loss']);\n",
    "        print(\"Group {}, Epoch {}: G loss: {} vs. C loss: {}\".format(group_id, 1+i, g_loss, c_loss));\n",
    "        \n",
    "        # delete the history to free memory\n",
    "        del history, history2\n",
    "        \n",
    "        # make a new set of notes\n",
    "        res_noise = np.random.random((1, g_input_size));\n",
    "        res_input = conv_input(res_noise, extvar);\n",
    "        _resgenerated, res_map, _resclass = mmodel.predict(res_input);\n",
    "        if plot_map:\n",
    "            plot_current_map(tf.convert_to_tensor(res_map, dtype=tf.float32));\n",
    "        \n",
    "        # early return if found a good solution\n",
    "        # good is (inside the map boundary)\n",
    "        if i >= good_epoch:\n",
    "#             current_map = construct_map_with_sliders(tf.convert_to_tensor(res, dtype=\"float32\"), extvar=extvar);\n",
    "            current_map = res_map;\n",
    "            if inblock_trueness(current_map[:, :, 0:2]).numpy()[0] == 0 and inblock_trueness(current_map[:, :, 4:6]).numpy()[0] == 0:\n",
    "                # debugging options to check map integrity\n",
    "#                 print(tf.reduce_mean(current_map));\n",
    "#                 print(\"-----MAPLAYER-----\")\n",
    "#                 print(tf.reduce_mean(mapping_layer(conv_input(tf.convert_to_tensor(_resgenerated, dtype=\"float32\"), extvar))));\n",
    "#                 print(\"-----CMWS-----\")\n",
    "#                 print(tf.reduce_mean(construct_map_with_sliders(tf.convert_to_tensor(_resgenerated, dtype=\"float32\"), extvar=mapping_layer.extvar)));\n",
    "                break;\n",
    "\n",
    "#     plot_history(history);\n",
    "#     plot_history(history2);\n",
    "    if plot_map:\n",
    "        for i in range(3): # from our testing, any random input generates nearly the same map\n",
    "            plot_noise = np.random.random((1, g_input_size));\n",
    "            plot_input = conv_input(plot_noise, extvar);\n",
    "            _plotgenerated, plot_mapped, _plotclass = mmodel.predict(plot_input);\n",
    "            plot_current_map(tf.convert_to_tensor(plot_mapped, dtype=tf.float32));\n",
    "    \n",
    "#     del mmodel, mapping_layer;\n",
    "    \n",
    "    return res_map.squeeze();\n",
    "#     onoise = np.random.random((1, g_input_size));\n",
    "    \n",
    "#     return construct_map_with_sliders(tf.convert_to_tensor(gmodel.predict(onoise)), extvar=extvar).numpy().squeeze();\n",
    "\n",
    "# generate the map (main function)\n",
    "# dist_multiplier in #6 is used here\n",
    "def generate_map():\n",
    "    o = [];\n",
    "    note_group_size = GAN_PARAMS[\"note_group_size\"];\n",
    "    pos = [np.random.randint(100, 412), np.random.randint(80, 304)];\n",
    "    models = make_models();\n",
    "    \n",
    "    print(\"# of groups: {}\".format(timestamps.shape[0] // note_group_size));\n",
    "    for i in range(timestamps.shape[0] // note_group_size):\n",
    "        z = generate_set(models, begin = i * note_group_size, start_pos = pos, length_multiplier = dist_multiplier, group_id = i, plot_map=False) * np.array([512, 384, 1, 1, 512, 384]);\n",
    "        pos = z[-1, 0:2];\n",
    "        o.append(z);\n",
    "    a = np.concatenate(o, axis=0);\n",
    "    return a;\n",
    "\n",
    "# generate a test map (debugging function)\n",
    "# dist_multiplier in #6 is used here\n",
    "def generate_test():\n",
    "    o = [];\n",
    "    pos = [384, 288];\n",
    "    note_group_size = GAN_PARAMS[\"note_group_size\"];\n",
    "    generate_set(begin = 3 * note_group_size, start_pos = pos, length_multiplier = dist_multiplier, group_id = 3, plot_map=True);\n",
    "\n",
    "# for debugging only! it should be sent to node load_map.js c instead.\n",
    "def print_osu_text(a):\n",
    "    for i, ai in enumerate(a):\n",
    "        if not is_slider[i]:\n",
    "            print(\"{},{},{},1,0,0:0:0\".format(int(ai[0]), int(ai[1]), int(timestamps[i])));\n",
    "        else:\n",
    "            print(\"{},{},{},2,0,L|{}:{},1,{},0:0:0\".format(int(ai[0]), int(ai[1]), int(timestamps[i]), int(round(ai[0] + ai[2] * slider_lengths[i])), int(round(ai[1] + ai[3] * slider_lengths[i])), int(slider_length_base[i] * slider_ticks[i])));\n",
    "    \n",
    "osu_a = generate_map();\n",
    "# generate_test();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now convert the generated flow data to a dict, and mix it into the JSON file converted from .osu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this needs to be JSON serializable, so we need to carefully convert the types.\n",
    "# Numpy somehow got its own types like numpy.int64 that does not allow the data to be serialized to JSON.\n",
    "def convert_to_osu_obj(obj_array):\n",
    "    output = [];\n",
    "    for i, obj in enumerate(obj_array):\n",
    "        if not is_slider[i]: # is a circle; does not consider spinner for now.\n",
    "            obj_dict = {\n",
    "                \"x\": int(obj[0]),\n",
    "                \"y\": int(obj[1]),\n",
    "                \"type\": 1,\n",
    "                \"time\": int(timestamps[i]),\n",
    "                \"hitsounds\": 0,\n",
    "                \"extHitsounds\": \"0:0:0\",\n",
    "                \"index\": i\n",
    "            };\n",
    "        else:\n",
    "            obj_dict = {\n",
    "                \"x\": int(obj[0]),\n",
    "                \"y\": int(obj[1]),\n",
    "                \"type\": 2,\n",
    "                \"time\": int(timestamps[i]),\n",
    "                \"hitsounds\": 0,\n",
    "                \"extHitsounds\": \"0:0:0\",\n",
    "                \"sliderGenerator\": {\n",
    "                    \"type\": int(slider_types[i]),\n",
    "                    \"dOut\": [float(obj[2]), float(obj[3])],\n",
    "                    \"len\": float(slider_length_base[i] * slider_ticks[i]),\n",
    "                    \"ticks\": int(slider_ticks[i]),\n",
    "                    \"endpoint\": [int(obj[4]), int(obj[5])]\n",
    "                },\n",
    "                \"index\": i\n",
    "            };\n",
    "        output.append(obj_dict);\n",
    "    return output;\n",
    "\n",
    "def get_osu_file_name(metadata):\n",
    "    artist = metadata[\"artist\"];\n",
    "    title = metadata[\"title\"];\n",
    "    creator = metadata[\"creator\"];\n",
    "    diffname = metadata[\"diffname\"];\n",
    "    outname = (artist+\" - \" if len(artist) > 0 else \"\") + title + \" (\" + creator + \") [\" + diffname + \"].osu\";\n",
    "    outname = re.sub(\"[^a-zA-Z0-9\\(\\)\\[\\] \\.\\,\\!\\~\\`\\{\\}\\-\\_\\=\\+\\&\\^\\@\\#\\$\\%\\;\\']\",\"\", outname);\n",
    "    return outname;\n",
    "\n",
    "osu_obj_array = convert_to_osu_obj(osu_a);\n",
    "\n",
    "with open(\"mapthis.json\", encoding=\"utf-8\") as map_json:\n",
    "    map_dict = json.load(map_json);\n",
    "    map_meta = map_dict[\"meta\"];\n",
    "    filename = get_osu_file_name(map_meta);\n",
    "    map_dict[\"obj\"] = osu_obj_array;\n",
    "\n",
    "with open('mapthis.json', 'w', encoding=\"utf-8\") as outfile:\n",
    "    json.dump(map_dict, outfile, ensure_ascii=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, ask node.js to convert the JSON back to a .osu file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success! finished on: 2019-12-01 04:08:48.293009\n"
     ]
    }
   ],
   "source": [
    "subprocess.call([\"node\", \"load_map.js\", \"c\", \"mapthis.json\", filename]);\n",
    "print(\"success! finished on: {}\".format(datetime.now()));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vPnIVuaSJwWz"
   },
   "source": [
    "If it works alright, you should have a nice .osu file under the folder of these notebooks now!\n",
    "\n",
    "If it does not work, please tell me the problem so probably I could fix it!\n",
    "\n",
    "@2019/6/29"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
